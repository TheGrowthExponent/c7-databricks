{
  "$schema": "https://context7.com/schema/context7.json",
  "projectTitle": "Databricks",
  "description": "Comprehensive Databricks documentation for Context7 - 30 files with 300+ code examples covering REST API, Python SDK, Delta Lake, MLflow, SQL, CLI, ETL patterns, ML workflows, streaming, and best practices",
  "folders": ["docs", "README.md", "CONTRIBUTING.md", "PROJECT-STATUS.md", "LICENSE"],
  "excludeFolders": [
    "node_modules",
    ".git",
    "dist",
    "build",
    "tests",
    "coverage",
    ".github",
    ".vscode",
    ".cursor",
    ".zed",
    "*.log",
    "*.tmp",
    "*.bak",
    "docs/plan.md",
    "docs/progress.md",
    "docs/extraction-strategy.md",
    "docs/session-summary.md",
    "docs/skills/SKILL-FORMAT-GUIDE.md",
    "docs/skills/SKILL-QUICK-REF.md",
    "docs/skills/SKILL-TEMPLATE.md",
    "docs/skills/SKILL.md",
    "docs/skills/*/SKILL.md",
    "VALIDATION-SYSTEM-DELIVERY.md",
    "AGENT-SYSTEM-DELIVERY.md",
    "AI-DEV-KIT-ANALYSIS.md",
    "COMPLETION-SUMMARY.md",
    "CONTEXT7-GUIDELINES-ADDED.md",
    "DATE-RULE-IMPLEMENTATION.md",
    "GETTING-STARTED-UPDATE.md",
    "ML-TUTORIALS-ANALYSIS.md",
    "NEXT-STEPS.md",
    "PHASE-4-VALIDATION.md",
    "PHASE-5-RECOMMENDATION.md",
    "SESSION-2026-01-15.md",
    "SKILL-FORMAT-IMPLEMENTATION.md",
    "SKILLS-DOCUMENTATION-SUMMARY.md",
    "ZED-CONFIGURATION.md",
    "docssdkmlflow.md"
  ],
  "rules": [
    "Use clear, concise documentation with production-ready examples",
    "Include comprehensive code examples for all scenarios",
    "Follow Databricks API conventions and best practices",
    "Document common use cases and advanced patterns",
    "Include error handling and security considerations in all examples",
    "Provide version-specific guidance (DBR 13.x, DBR 12.x)",
    "Show real-world patterns for ETL, ML, and streaming workflows",
    "Include IDE integration and local development setup with Databricks Connect",
    "Use databricks-sdk Python package for API interactions",
    "Implement proper authentication using environment variables or Azure Key Vault",
    "Follow Delta Lake best practices for ACID transactions and optimization",
    "Use MLflow for experiment tracking and model registry",
    "Implement structured streaming with proper checkpointing and error handling",
    "Use Unity Catalog for centralized data governance and access control",
    "Follow security best practices including credential management and network isolation",
    "Optimize Spark jobs with caching, partitioning, and broadcast joins",
    "Use cluster policies to control costs and enforce security standards",
    "Implement proper logging and monitoring for production workloads"
  ]
}
